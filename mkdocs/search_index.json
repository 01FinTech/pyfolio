{
    "docs": [
        {
            "location": "/",
            "text": "pyfolio\n\n\n\n\n\n\npyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by\n\nQuantopian Inc\n. It works well with the\n\nZipline\n open source backtesting library.\n\n\nAt the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a the Zipline algo included as one of our example notebooks:\n\n\n\n\n\n\n\n\n\n\nSee also \nslides of a recent talk about pyfolio.\n\n\nInstallation\n\n\nTo install \npyfolio\n via \npip\n issue the following command:\n\n\npip install pyfolio\n\n\n\n\nFor development, clone the git repo and run \npython setup.py develop\n\nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.\n\n\npyfolio\n has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n - \nseaborn\n\n - \npymc3\n (optional)\n - \nzipline\n (optional; requires master, \nnot\n 0.7.0)\n\n\nQuestions?\n\n\nIf you find a bug, feel free to open an issue on our github tracker.\n\n\nYou can also join our \nmailing list\n.\n\n\nContribute\n\n\nIf you want to contribute, a great place to start would be the \nhelp-wanted issues\n.\n\n\nCredits\n\n\n\n\nGus Gordon (gus@quantopian.com)\n\n\nJustin Lent (justin@quantopian.com)\n\n\nSepideh Sadeghi (sp.sadeghi@gmail.com)\n\n\nThomas Wiecki (thomas@quantopian.com)\n\n\nJessica Stauth (jstauth@quantopian.com)\n\n\nKaren Rubin (karen@quantopian.com)\n\n\nDavid Edwards (dedwards@quantopian.com)\n\n\n\n\nFor a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.",
            "title": "Overview"
        },
        {
            "location": "/#pyfolio",
            "text": "pyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by Quantopian Inc . It works well with the Zipline  open source backtesting library.  At the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a the Zipline algo included as one of our example notebooks:      See also  slides of a recent talk about pyfolio.",
            "title": "pyfolio"
        },
        {
            "location": "/#installation",
            "text": "To install  pyfolio  via  pip  issue the following command:  pip install pyfolio  For development, clone the git repo and run  python setup.py develop \nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.  pyfolio  has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n -  seaborn \n -  pymc3  (optional)\n -  zipline  (optional; requires master,  not  0.7.0)",
            "title": "Installation"
        },
        {
            "location": "/#questions",
            "text": "If you find a bug, feel free to open an issue on our github tracker.  You can also join our  mailing list .",
            "title": "Questions?"
        },
        {
            "location": "/#contribute",
            "text": "If you want to contribute, a great place to start would be the  help-wanted issues .",
            "title": "Contribute"
        },
        {
            "location": "/#credits",
            "text": "Gus Gordon (gus@quantopian.com)  Justin Lent (justin@quantopian.com)  Sepideh Sadeghi (sp.sadeghi@gmail.com)  Thomas Wiecki (thomas@quantopian.com)  Jessica Stauth (jstauth@quantopian.com)  Karen Rubin (karen@quantopian.com)  David Edwards (dedwards@quantopian.com)   For a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.",
            "title": "Credits"
        },
        {
            "location": "/whatsnew/",
            "text": "What's New\n\n\nThese are new features and improvements of note in each release.\n\n\nv0.3 (Oct 23, 2015)\n\n\nThis is a major release from 0.2 that includes many exciting new features. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nSector exposures: sum positions by sector given a dictionary or series of symbol to sector mappings \nPR166\n\n\nAbility to make cones with multiple shades stdev regions \nPR168\n\n\nSlippage sweep: See how an algorithm performs with various levels of slippage \nPR170\n\n\nStochastic volatility model in Bayesian tear sheet \nPR174\n\n\nAbility to suppress display of position information \nPR177\n\n\n\n\nBug fixes\n\n\n\n\nVarious fixes to make pyfolio pandas 0.17 compatible\n\n\n\n\nv0.2 (Oct 16, 2015)\n\n\nThis is a major release from 0.1 that includes mainly bugfixes and refactorings but also some new features. We recommend that all users upgrade to this new version.\n\n\nNew features\n\n\n\n\nVolatility matched cumulative returns plot \nPR126\n.\n\n\nAllow for different periodicity (annualization factors) in the annual_() methods \nPR164\n.\n\n\nUsers can supply their own interesting periods \nPR163\n.\n\n\nAbility to weight a portfolio of holdings by a metric valued \nPR161\n.\n\n\n\n\nBug fixes\n\n\n\n\nFix drawdown overlaps \nPR150\n.\n\n\nMonthly returns distribution should not stack by year \nPR162\n.\n\n\nFix gross leverage \nPR147",
            "title": "Releases"
        },
        {
            "location": "/whatsnew/#whats-new",
            "text": "These are new features and improvements of note in each release.",
            "title": "What's New"
        },
        {
            "location": "/whatsnew/#v03-oct-23-2015",
            "text": "This is a major release from 0.2 that includes many exciting new features. We recommend that all users upgrade to this new version.  New features   Sector exposures: sum positions by sector given a dictionary or series of symbol to sector mappings  PR166  Ability to make cones with multiple shades stdev regions  PR168  Slippage sweep: See how an algorithm performs with various levels of slippage  PR170  Stochastic volatility model in Bayesian tear sheet  PR174  Ability to suppress display of position information  PR177   Bug fixes   Various fixes to make pyfolio pandas 0.17 compatible",
            "title": "v0.3 (Oct 23, 2015)"
        },
        {
            "location": "/whatsnew/#v02-oct-16-2015",
            "text": "This is a major release from 0.1 that includes mainly bugfixes and refactorings but also some new features. We recommend that all users upgrade to this new version.  New features   Volatility matched cumulative returns plot  PR126 .  Allow for different periodicity (annualization factors) in the annual_() methods  PR164 .  Users can supply their own interesting periods  PR163 .  Ability to weight a portfolio of holdings by a metric valued  PR161 .   Bug fixes   Fix drawdown overlaps  PR150 .  Monthly returns distribution should not stack by year  PR162 .  Fix gross leverage  PR147",
            "title": "v0.2 (Oct 16, 2015)"
        },
        {
            "location": "/single_stock_example/",
            "text": "Single stock analysis example in pyfolio\n\n\nHere's a simple example where we produce a set of plots, called a tear sheet, for a stock.\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nFetch the daily returns for a stock\n\n\nstock_rets = pf.utils.get_symbol_rets('FB')\n\n\n\n\nCreate a full tear sheet for the single stock\n\n\nThis will show charts about returns and shock events.\n\n\npf.create_returns_tear_sheet(stock_rets)\n\n\n\n\nEntire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nCreate a full tear sheet for an equal-weight portfolio of:\n\n\n\n\nLong SPY\n\n\nShort QQQ\n\n\nLong GLD\n\n\nLong TLT\n\n\n\n\nAdditionally, we set the live start date as an example.\n\n\nstock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')\n\n\n\n\nportfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)\n\n\n\n\npf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')\n\n\n\n\nEntire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007",
            "title": "Single stock"
        },
        {
            "location": "/single_stock_example/#single-stock-analysis-example-in-pyfolio",
            "text": "Here's a simple example where we produce a set of plots, called a tear sheet, for a stock.",
            "title": "Single stock analysis example in pyfolio"
        },
        {
            "location": "/single_stock_example/#import-pyfolio",
            "text": "%matplotlib inline\nimport pyfolio as pf",
            "title": "Import pyfolio"
        },
        {
            "location": "/single_stock_example/#fetch-the-daily-returns-for-a-stock",
            "text": "stock_rets = pf.utils.get_symbol_rets('FB')",
            "title": "Fetch the daily returns for a stock"
        },
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-the-single-stock",
            "text": "This will show charts about returns and shock events.  pf.create_returns_tear_sheet(stock_rets)  Entire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)",
            "title": "Create a full tear sheet for the single stock"
        },
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-an-equal-weight-portfolio-of",
            "text": "Long SPY  Short QQQ  Long GLD  Long TLT   Additionally, we set the live start date as an example.  stock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')  portfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)  pf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')  Entire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007",
            "title": "Create a full tear sheet for an equal-weight portfolio of:"
        },
        {
            "location": "/zipline_algo_example/",
            "text": "Zipline algorithm analysis example in pyfolio\n\n\nHere's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.\n\n\nImports\n\n\nImport pyfolio, along with the necessary modules for running our zipline backtest.\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/pandas/io/data.py:33: FutureWarning: \nThe pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\nAfter installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n  FutureWarning)\n\n\n\nimport numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission\n\n\n\n\nRun our zipline algorithm\n\n\nThis algorithm can also be adjusted to execute a modified, or completely different, trading strategy.\n\n\n# Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days \n algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n    \nProjection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z \n 0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n    \n proj = simplex_projection([.4 ,.3, -.4, .5])\n    \n print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n    \n print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n    \n\n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v \n 0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u \n (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w \n 0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)\n\n\n\n\nAMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-10-21 14:10:42.252494] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-10-21 14:10:42.253271] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-10-21 14:10:42.253904] INFO: Performance: last close: 2009-12-31 21:00:00+00:00\n\n\n\nExtract metrics\n\n\nGet the returns, positions, and transactions from the zipline backtest object.\n\n\nreturns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)\n\n\n\n\nSingle plot example\n\n\nMake one plot of the top 5 drawdown periods.\n\n\npf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')\n\n\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\n\nmatplotlib.text.Text at 0x7f7287bb0860\n\n\n\n\n\n\nFull tear sheet example\n\n\nCreate a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.\n\n\npf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')\n\n\n\n\nEntire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062\n\n\n\n\n\nTop 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\n\n\n\n\n\nSuppressing symbol output\n\n\nWhen sharing tear sheets it might be undesirable to display which symbols where used by a strategy. To suppress these in the tear sheet you can pass \nhide_positions=True\n.\n\n\npf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22',\n                          hide_positions=True)\n\n\n\n\nEntire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062",
            "title": "Zipline algorithm"
        },
        {
            "location": "/zipline_algo_example/#zipline-algorithm-analysis-example-in-pyfolio",
            "text": "Here's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.",
            "title": "Zipline algorithm analysis example in pyfolio"
        },
        {
            "location": "/zipline_algo_example/#imports",
            "text": "Import pyfolio, along with the necessary modules for running our zipline backtest.  %matplotlib inline\nimport pyfolio as pf  /home/wiecki/miniconda3/lib/python3.4/site-packages/pandas/io/data.py:33: FutureWarning: \nThe pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\nAfter installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n  FutureWarning)  import numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission",
            "title": "Imports"
        },
        {
            "location": "/zipline_algo_example/#run-our-zipline-algorithm",
            "text": "This algorithm can also be adjusted to execute a modified, or completely different, trading strategy.  # Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days   algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n     Projection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z   0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n      proj = simplex_projection([.4 ,.3, -.4, .5])\n      print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n      print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n     \n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v   0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u   (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w   0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)  AMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-10-21 14:10:42.252494] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-10-21 14:10:42.253271] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-10-21 14:10:42.253904] INFO: Performance: last close: 2009-12-31 21:00:00+00:00",
            "title": "Run our zipline algorithm"
        },
        {
            "location": "/zipline_algo_example/#extract-metrics",
            "text": "Get the returns, positions, and transactions from the zipline backtest object.  returns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)",
            "title": "Extract metrics"
        },
        {
            "location": "/zipline_algo_example/#single-plot-example",
            "text": "Make one plot of the top 5 drawdown periods.  pf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')  /home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1) matplotlib.text.Text at 0x7f7287bb0860",
            "title": "Single plot example"
        },
        {
            "location": "/zipline_algo_example/#full-tear-sheet-example",
            "text": "Create a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.  pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')  Entire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)   Stress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062   Top 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]",
            "title": "Full tear sheet example"
        },
        {
            "location": "/zipline_algo_example/#suppressing-symbol-output",
            "text": "When sharing tear sheets it might be undesirable to display which symbols where used by a strategy. To suppress these in the tear sheet you can pass  hide_positions=True .  pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22',\n                          hide_positions=True)  Entire data start date: 2004-01-02\nEntire data end date: 2009-12-31\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nannual_return          0.12           0.06         0.12\nannual_volatility      0.26           0.22         0.25\nsharpe_ratio           0.47           0.28         0.48\ncalmar_ratio           0.20           0.84         0.21\nstability              0.00           0.05         0.01\nmax_drawdown          -0.60          -0.07        -0.60\nomega_ratio            1.09           1.05         1.09\nsortino_ratio          0.70           0.39         0.71\nskewness               0.28          -0.28         0.27\nkurtosis               4.10           0.46         4.05\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %  peak date valley date recovery date duration\n0              59.50 2007-11-06  2008-11-20           NaT      NaN\n1              22.33 2006-02-16  2006-08-31    2007-05-21      328\n2              12.52 2005-07-28  2005-10-12    2006-01-11      120\n3              11.28 2004-11-15  2005-04-28    2005-07-22      180\n4               9.44 2007-07-16  2007-08-06    2007-09-04       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)   Stress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062",
            "title": "Suppressing symbol output"
        },
        {
            "location": "/bayesian/",
            "text": "Bayesian performance analysis example in pyfolio\n\n\nThere are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics. \n\n\nThe main benefit of these methods is \nuncertainty quantification\n. All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.\n\n\nLets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in \nPyMC3\n to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in \ncreate_full_tear_sheet()\n).\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nFetch the daily returns for a stock\n\n\nstock_rets = pf.utils.get_symbol_rets('FB')\n\n\n\n\nCreate Bayesian tear sheet\n\n\nout_of_sample = stock_rets.index[-40]\n\n\n\n\npf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample, stoch_vol=True)\n\n\n\n\nRunning T model\n [-----------------100%-----------------] 2000 of 2000 complete in 7.0 sec\nFinished T model (required 27.89 seconds).\n\nRunning BEST model\n [-----------------100%-----------------] 2000 of 2000 complete in 7.1 sec\nFinished BEST model (required 59.35 seconds).\n\nFinished plotting Bayesian cone (required 0.10 seconds).\n\nFinished plotting BEST results (required 0.73 seconds).\n\nFinished computing Bayesian predictions (required 0.14 seconds).\n\nFinished plotting Bayesian VaRs estimate (required 0.07 seconds).\n\nRunning alpha beta model\n [-----------------100%-----------------] 2000 of 2000 complete in 4.4 sec\nFinished running alpha beta model (required 30.92 seconds).\n\nFinished plotting alpha beta model (required 0.17 seconds).\n\nRunning stochastic volatility model on most recent 400 days of returns\n\nFinished running stochastic volatility model (required 88.54 seconds).\n\nPlotting stochastic volatility model\n\nFinished plotting stochastic volatility model (required 0.41 seconds).\n\nTotal runtime was 208.33 seconds.\n\n\n\n\n\nLets go through these row by row:\n\n\n\n\nThe first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a \nStudent-T distribution\n with heavier tails.\n\n\nThe next row compares mean returns of the in-sample (backest) and out-of-sample or OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. The green distribution on the left side is much wider, representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability \n 97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called \nBEST\n and was developed by John Kruschke.\n\n\nThe next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.\n\n\nThe 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.\n\n\nThe 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.\n\n\nThe 7th row shows a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.\n\n\nThe 8th row shows Bayesian estimates for log(sigma) and log(nu), two parameters of the \nstochastic volatility model\n.\n\n\nThe last row shows the volatility measured by the stochastic voatility model, overlaid on the absolute value of the returns.\n\n\nNote that the last two rows are only shown when stoch_vol=True.  By default, stoch_vol=False because running the stochastic volatility model is computationally expensive.\n\n\nFinally, only the most recent 400 days of returns are used when computing the stochastic volatility model.  This is to minimize computational time.\n\n\n\n\nRunning models directly\n\n\nYou can also run individual models. All models can be found in \npyfolio.bayesian\n and run via the \nrun_model()\n function.\n\n\nhelp(pf.bayesian.run_model)\n\n\n\n\nHelp on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n\n\nFor example, to run a model that assumes returns to be normally distributed, you can call:\n\n\n# Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)\n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 0.9 sec\n\n\n\nThe returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are \n 0:\n\n\n# Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio \n 0 = {:3}%'.format((trace['sharpe'] \n 0).mean() * 100))\n\n\n\n\nProbability of Sharpe ratio \n 0 = 85.0%\n\n\n\nBut we can also interact with it like with any other \npymc3\n trace:\n\n\nimport pymc3 as pm\npm.traceplot(trace);\n\n\n\n\n\n\nFurther reading\n\n\nFor more information on Bayesian statistics, check out these resources:\n\n\n\n\nA blog post about the Bayesian models with Sepideh Sadeghi\n\n\nMy personal blog on Bayesian modeling\n\n\nA talk I gave in Singapore on \nProbabilistic Programming in Quantitative Finance\n\n\nThe IPython NB book \nBayesian Methods for Hackers\n.",
            "title": "Bayesian analysis"
        },
        {
            "location": "/bayesian/#bayesian-performance-analysis-example-in-pyfolio",
            "text": "There are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics.   The main benefit of these methods is  uncertainty quantification . All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.  Lets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in  PyMC3  to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in  create_full_tear_sheet() ).",
            "title": "Bayesian performance analysis example in pyfolio"
        },
        {
            "location": "/bayesian/#import-pyfolio",
            "text": "%matplotlib inline\nimport pyfolio as pf",
            "title": "Import pyfolio"
        },
        {
            "location": "/bayesian/#fetch-the-daily-returns-for-a-stock",
            "text": "stock_rets = pf.utils.get_symbol_rets('FB')",
            "title": "Fetch the daily returns for a stock"
        },
        {
            "location": "/bayesian/#create-bayesian-tear-sheet",
            "text": "out_of_sample = stock_rets.index[-40]  pf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample, stoch_vol=True)  Running T model\n [-----------------100%-----------------] 2000 of 2000 complete in 7.0 sec\nFinished T model (required 27.89 seconds).\n\nRunning BEST model\n [-----------------100%-----------------] 2000 of 2000 complete in 7.1 sec\nFinished BEST model (required 59.35 seconds).\n\nFinished plotting Bayesian cone (required 0.10 seconds).\n\nFinished plotting BEST results (required 0.73 seconds).\n\nFinished computing Bayesian predictions (required 0.14 seconds).\n\nFinished plotting Bayesian VaRs estimate (required 0.07 seconds).\n\nRunning alpha beta model\n [-----------------100%-----------------] 2000 of 2000 complete in 4.4 sec\nFinished running alpha beta model (required 30.92 seconds).\n\nFinished plotting alpha beta model (required 0.17 seconds).\n\nRunning stochastic volatility model on most recent 400 days of returns\n\nFinished running stochastic volatility model (required 88.54 seconds).\n\nPlotting stochastic volatility model\n\nFinished plotting stochastic volatility model (required 0.41 seconds).\n\nTotal runtime was 208.33 seconds.   Lets go through these row by row:   The first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a  Student-T distribution  with heavier tails.  The next row compares mean returns of the in-sample (backest) and out-of-sample or OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. The green distribution on the left side is much wider, representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability   97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called  BEST  and was developed by John Kruschke.  The next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.  The 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.  The 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.  The 7th row shows a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.  The 8th row shows Bayesian estimates for log(sigma) and log(nu), two parameters of the  stochastic volatility model .  The last row shows the volatility measured by the stochastic voatility model, overlaid on the absolute value of the returns.  Note that the last two rows are only shown when stoch_vol=True.  By default, stoch_vol=False because running the stochastic volatility model is computationally expensive.  Finally, only the most recent 400 days of returns are used when computing the stochastic volatility model.  This is to minimize computational time.",
            "title": "Create Bayesian tear sheet"
        },
        {
            "location": "/bayesian/#running-models-directly",
            "text": "You can also run individual models. All models can be found in  pyfolio.bayesian  and run via the  run_model()  function.  help(pf.bayesian.run_model)  Help on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.  For example, to run a model that assumes returns to be normally distributed, you can call:  # Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)   [-----------------100%-----------------] 500 of 500 complete in 0.9 sec  The returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are   0:  # Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio   0 = {:3}%'.format((trace['sharpe']   0).mean() * 100))  Probability of Sharpe ratio   0 = 85.0%  But we can also interact with it like with any other  pymc3  trace:  import pymc3 as pm\npm.traceplot(trace);",
            "title": "Running models directly"
        },
        {
            "location": "/bayesian/#further-reading",
            "text": "For more information on Bayesian statistics, check out these resources:   A blog post about the Bayesian models with Sepideh Sadeghi  My personal blog on Bayesian modeling  A talk I gave in Singapore on  Probabilistic Programming in Quantitative Finance  The IPython NB book  Bayesian Methods for Hackers .",
            "title": "Further reading"
        }
    ]
}