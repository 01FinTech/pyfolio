{
    "docs": [
        {
            "location": "/",
            "text": "pyfolio\n\n\n\n\n\n\npyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by\n\nQuantopian Inc\n. It works well with the\n\nZipline\n open source backtesting library.\n\n\nAt the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a returns-based analysis of the \n$FB\n stock:\n\n\n\n\nInstallation\n\n\nTo install \npyfolio\n via \npip\n issue the following command:\n\n\npip install pyfolio\n\n\n\n\nFor development, clone the git repo and run \npython setup.py develop\n\nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.\n\n\npyfolio\n has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n - \nseaborn\n\n - \npymc3\n (optional)\n\n\nQuestions?\n\n\nIf you find a bug, feel free to open an issue on our github tracker.\n\n\nYou can also join our \nmailing list\n.\n\n\nContribute\n\n\nIf you want to contribute, a great place to start would be the \nhelp-wanted issues\n.\n\n\nCredits\n\n\n\n\nGus Gordon (gus@quantopian.com)\n\n\nJustin Lent (justin@quantopian.com)\n\n\nSepideh Sadeghi (sp.sadeghi@gmail.com)\n\n\nThomas Wiecki (thomas@quantopian.com)\n\n\nJessica Stauth (jstauth@quantopian.com)\n\n\nKaren Rubin (karen@quantopian.com)\n\n\nDavid Edwards (dedwards@quantopian.com)\n\n\n\n\nFor a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.",
            "title": "Overview"
        },
        {
            "location": "/#pyfolio",
            "text": "pyfolio is a Python library for performance and risk analysis of\nfinancial portfolios developed by Quantopian Inc . It works well with the Zipline  open source backtesting library.  At the core of pyfolio is a so-called tear sheet that consists of\nvarious individual plots that provide a comprehensive image of the\nperformance of a trading algorithm. Here is an example of a tear sheet of a returns-based analysis of the  $FB  stock:",
            "title": "pyfolio"
        },
        {
            "location": "/#installation",
            "text": "To install  pyfolio  via  pip  issue the following command:  pip install pyfolio  For development, clone the git repo and run  python setup.py develop \nand edit the library files directly. Make sure to reload or restart\nthe IPython kernel when you make changes.  pyfolio  has the following dependencies:\n - numpy\n - scipy\n - pandas\n - matplotlib\n -  seaborn \n -  pymc3  (optional)",
            "title": "Installation"
        },
        {
            "location": "/#questions",
            "text": "If you find a bug, feel free to open an issue on our github tracker.  You can also join our  mailing list .",
            "title": "Questions?"
        },
        {
            "location": "/#contribute",
            "text": "If you want to contribute, a great place to start would be the  help-wanted issues .",
            "title": "Contribute"
        },
        {
            "location": "/#credits",
            "text": "Gus Gordon (gus@quantopian.com)  Justin Lent (justin@quantopian.com)  Sepideh Sadeghi (sp.sadeghi@gmail.com)  Thomas Wiecki (thomas@quantopian.com)  Jessica Stauth (jstauth@quantopian.com)  Karen Rubin (karen@quantopian.com)  David Edwards (dedwards@quantopian.com)   For a full list of contributors, see https://github.com/quantopian/pyfolio/graphs/contributors.",
            "title": "Credits"
        },
        {
            "location": "/single_stock_example/",
            "text": "Single stock analysis example in pyfolio\n\n\nHere's a simple example where we produce a set of plots, called a tear sheet, for a stock.\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nFetch the daily returns for a stock\n\n\nstock_rets = pf.utils.get_symbol_rets('FB')\n\n\n\n\nCreate a full tear sheet for the single stock\n\n\nThis will show charts about returns and shock events.\n\n\npf.create_returns_tear_sheet(stock_rets)\n\n\n\n\nEntire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\nCreate a full tear sheet for an equal-weight portfolio of:\n\n\n\n\nLong SPY\n\n\nShort QQQ\n\n\nLong GLD\n\n\nLong TLT\n\n\n\n\nAdditionally, we set the live start date as an example.\n\n\nstock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')\n\n\n\n\nportfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)\n\n\n\n\npf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')\n\n\n\n\nEntire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007",
            "title": "Single stock"
        },
        {
            "location": "/single_stock_example/#single-stock-analysis-example-in-pyfolio",
            "text": "Here's a simple example where we produce a set of plots, called a tear sheet, for a stock.",
            "title": "Single stock analysis example in pyfolio"
        },
        {
            "location": "/single_stock_example/#import-pyfolio",
            "text": "%matplotlib inline\nimport pyfolio as pf",
            "title": "Import pyfolio"
        },
        {
            "location": "/single_stock_example/#fetch-the-daily-returns-for-a-stock",
            "text": "stock_rets = pf.utils.get_symbol_rets('FB')",
            "title": "Fetch the daily returns for a stock"
        },
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-the-single-stock",
            "text": "This will show charts about returns and shock events.  pf.create_returns_tear_sheet(stock_rets)  Entire data start date: 2012-05-21 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nBacktest Months: 38\n                   Backtest\nsortino_ratio          1.65\nomega_ratio            1.18\nmax_drawdown          -0.48\ncalmar_ratio           0.79\nannual_return          0.38\nstability              0.87\nsharpe_ratio           0.85\nannual_volatility      0.44\nalpha                  0.20\nbeta                   0.98\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              47.90  2012-05-21 00:00:00+00:00  2012-09-04 00:00:00+00:00   \n1              22.06  2014-03-10 00:00:00+00:00  2014-04-28 00:00:00+00:00   \n2              17.34  2013-10-18 00:00:00+00:00  2013-11-25 00:00:00+00:00   \n4               9.21  2014-10-28 00:00:00+00:00  2014-11-19 00:00:00+00:00   \n3               9.20  2015-03-24 00:00:00+00:00  2015-05-12 00:00:00+00:00\n\n               recovery date duration  \n0  2013-07-25 00:00:00+00:00      309  \n1  2014-07-24 00:00:00+00:00       99  \n2  2013-12-17 00:00:00+00:00       43  \n4  2014-12-22 00:00:00+00:00       40  \n3  2015-06-23 00:00:00+00:00       66\n\n\n2-sigma returns daily    -0.054\n2-sigma returns weekly   -0.110\ndtype: float64\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)",
            "title": "Create a full tear sheet for the single stock"
        },
        {
            "location": "/single_stock_example/#create-a-full-tear-sheet-for-an-equal-weight-portfolio-of",
            "text": "Long SPY  Short QQQ  Long GLD  Long TLT   Additionally, we set the live start date as an example.  stock_rets_SPY = pf.utils.get_symbol_rets('SPY')\nstock_rets_QQQ = pf.utils.get_symbol_rets('QQQ')\nstock_rets_GLD = pf.utils.get_symbol_rets('GLD')\nstock_rets_TLT = pf.utils.get_symbol_rets('TLT')  portfolio_4_assets_rets = pf.timeseries.portfolio_returns([stock_rets_SPY, -1 * stock_rets_QQQ, stock_rets_GLD, stock_rets_TLT], \n                                exclude_non_overlapping=True)  pf.create_full_tear_sheet(portfolio_4_assets_rets, live_start_date='2013-10-22')  Entire data start date: 2004-11-19 00:00:00+00:00\nEntire data end date: 2015-08-04 00:00:00+00:00\n\n\nOut-of-Sample Months: 21\nBacktest Months: 106\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.96          -0.43         0.78\nomega_ratio            1.12           0.95         1.09\nmax_drawdown          -0.11          -0.08        -0.13\ncalmar_ratio           0.41          -0.20         0.28\nannual_return          0.05          -0.02         0.04\nstability              0.94           0.04         0.83\nsharpe_ratio           0.65          -0.28         0.53\nannual_volatility      0.07           0.06         0.07\nalpha                  0.05           0.00         0.04\nbeta                  -0.04          -0.17        -0.05\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              13.06  2012-11-13 00:00:00+00:00  2013-12-19 00:00:00+00:00   \n2               8.96  2008-03-17 00:00:00+00:00  2008-08-15 00:00:00+00:00   \n1               8.85  2008-12-30 00:00:00+00:00  2009-07-29 00:00:00+00:00   \n4               4.66  2010-10-06 00:00:00+00:00  2011-01-27 00:00:00+00:00   \n3               4.38  2012-01-31 00:00:00+00:00  2012-04-03 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n2  2008-12-10 00:00:00+00:00      193  \n1  2010-06-07 00:00:00+00:00      375  \n4  2011-04-27 00:00:00+00:00      146  \n3  2012-06-01 00:00:00+00:00       89\n\n\n2-sigma returns daily    -0.008\n2-sigma returns weekly   -0.016\ndtype: float64\n\nStress Events\n                                    mean    min    max\nLehmann                            0.001 -0.016  0.031\nUS downgrade/European Debt Crisis  0.002 -0.018  0.015\nFukushima                          0.001 -0.006  0.007\nEZB IR Event                       0.000 -0.007  0.004\nAug07                              0.000 -0.006  0.005\nSept08                             0.002 -0.016  0.031\n2009Q1                            -0.001 -0.012  0.015\n2009Q2                            -0.000 -0.012  0.020\nFlash Crash                        0.002 -0.009  0.015\nApr14                              0.000 -0.005  0.009\nOct14                             -0.000 -0.007  0.007",
            "title": "Create a full tear sheet for an equal-weight portfolio of:"
        },
        {
            "location": "/zipline_algo_example/",
            "text": "Zipline algorithm analysis example in pyfolio\n\n\nHere's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.\n\n\nImports\n\n\nImport pyfolio, along with the necessary modules for running our zipline backtest.\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission\n\n\n\n\nRun our zipline algorithm\n\n\nThis algorithm can also be adjusted to execute a modified, or completely different, trading strategy.\n\n\n# Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG, bubble=True),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days \n algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n    \nProjection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z \n 0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n    \n proj = simplex_projection([.4 ,.3, -.4, .5])\n    \n print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n    \n print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n    \n\n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v \n 0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u \n (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w \n 0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)\n\n\n\n\nAMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-08-05 20:16:25.197696] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-08-05 20:16:25.198356] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-08-05 20:16:25.198812] INFO: Performance: last close: 2009-12-31 21:00:00+00:00\n\n\n\nExtract metrics\n\n\nGet the returns, positions, and transactions from the zipline backtest object.\n\n\nreturns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)\n\n\n\n\nSingle plot example\n\n\nMake one plot of the top 5 drawdown periods.\n\n\npf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')\n\n\n\n\n/opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1)\n\n\n\n\n\n\nmatplotlib.text.Text at 0x7f645912e110\n\n\n\n\n\n\nFull tear sheet example\n\n\nCreate a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.\n\n\npf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')\n\n\n\n\nEntire data start date: 2004-01-02 00:00:00+00:00\nEntire data end date: 2009-12-31 00:00:00+00:00\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.74           0.43         0.76\nomega_ratio            1.09           1.05         1.09\nmax_drawdown          -0.60          -0.07        -0.60\ncalmar_ratio           0.20           0.84         0.21\nannual_return          0.12           0.06         0.12\nstability              0.00           0.05         0.01\nsharpe_ratio           0.47           0.28         0.48\nannual_volatility      0.26           0.22         0.25\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              59.50  2007-11-06 00:00:00+00:00  2008-11-20 00:00:00+00:00   \n1              22.33  2006-02-16 00:00:00+00:00  2006-08-31 00:00:00+00:00   \n2              12.52  2005-07-28 00:00:00+00:00  2005-10-12 00:00:00+00:00   \n3              11.28  2004-11-15 00:00:00+00:00  2005-04-28 00:00:00+00:00   \n4               9.44  2007-07-16 00:00:00+00:00  2007-08-06 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n1  2007-05-21 00:00:00+00:00      328  \n2  2006-01-11 00:00:00+00:00      120  \n3  2005-07-22 00:00:00+00:00      180  \n4  2007-09-04 00:00:00+00:00       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062\n\n\nTop 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]",
            "title": "Zipline algorithm"
        },
        {
            "location": "/zipline_algo_example/#zipline-algorithm-analysis-example-in-pyfolio",
            "text": "Here's an example where we run an algorithm with zipline, then produce tear sheets for that algorithm.",
            "title": "Zipline algorithm analysis example in pyfolio"
        },
        {
            "location": "/zipline_algo_example/#imports",
            "text": "Import pyfolio, along with the necessary modules for running our zipline backtest.  %matplotlib inline\nimport pyfolio as pf  import numpy as np\nimport pandas as pd\n\nimport sys\nimport logbook\nimport numpy as np\nfrom datetime import datetime\nimport pytz\n\n# Import Zipline, the open source backtester\nfrom zipline import TradingAlgorithm\nfrom zipline.data.loader import load_bars_from_yahoo\nfrom zipline.api import order_target, symbol, history, add_history, schedule_function, date_rules, time_rules\nfrom zipline.algorithm import TradingAlgorithm\nfrom zipline.utils.factory import load_from_yahoo\nfrom zipline.finance import commission",
            "title": "Imports"
        },
        {
            "location": "/zipline_algo_example/#run-our-zipline-algorithm",
            "text": "This algorithm can also be adjusted to execute a modified, or completely different, trading strategy.  # Zipline trading algorithm\n# Taken from zipline.examples.olmar\nzipline_logging = logbook.NestedSetup([\n    logbook.NullHandler(level=logbook.DEBUG, bubble=True),\n    logbook.StreamHandler(sys.stdout, level=logbook.INFO),\n    logbook.StreamHandler(sys.stderr, level=logbook.ERROR),\n])\nzipline_logging.push_application()\n\nSTOCKS = ['AMD', 'CERN', 'COST', 'DELL', 'GPS', 'INTC', 'MMM']\n\n\n# On-Line Portfolio Moving Average Reversion\n\n# More info can be found in the corresponding paper:\n# http://icml.cc/2012/papers/168.pdf\ndef initialize(algo, eps=1, window_length=5):\n    algo.stocks = STOCKS\n    algo.sids = [algo.symbol(symbol) for symbol in algo.stocks]\n    algo.m = len(algo.stocks)\n    algo.price = {}\n    algo.b_t = np.ones(algo.m) / algo.m\n    algo.last_desired_port = np.ones(algo.m) / algo.m\n    algo.eps = eps\n    algo.init = True\n    algo.days = 0\n    algo.window_length = window_length\n    algo.add_transform('mavg', 5)\n\n    algo.set_commission(commission.PerShare(cost=0))\n\n\ndef handle_data(algo, data):\n    algo.days += 1\n    if algo.days   algo.window_length:\n        return\n\n    if algo.init:\n        rebalance_portfolio(algo, data, algo.b_t)\n        algo.init = False\n        return\n\n    m = algo.m\n\n    x_tilde = np.zeros(m)\n    b = np.zeros(m)\n\n    # find relative moving average price for each asset\n    for i, sid in enumerate(algo.sids):\n        price = data[sid].price\n        # Relative mean deviation\n        x_tilde[i] = data[sid].mavg(algo.window_length) / price\n\n    ###########################\n    # Inside of OLMAR (algo 2)\n    x_bar = x_tilde.mean()\n\n    # market relative deviation\n    mark_rel_dev = x_tilde - x_bar\n\n    # Expected return with current portfolio\n    exp_return = np.dot(algo.b_t, x_tilde)\n    weight = algo.eps - exp_return\n    variability = (np.linalg.norm(mark_rel_dev)) ** 2\n\n    # test for divide-by-zero case\n    if variability == 0.0:\n        step_size = 0\n    else:\n        step_size = max(0, weight / variability)\n\n    b = algo.b_t + step_size * mark_rel_dev\n    b_norm = simplex_projection(b)\n    np.testing.assert_almost_equal(b_norm.sum(), 1)\n\n    rebalance_portfolio(algo, data, b_norm)\n\n    # update portfolio\n    algo.b_t = b_norm\n\n\ndef rebalance_portfolio(algo, data, desired_port):\n    # rebalance portfolio\n    desired_amount = np.zeros_like(desired_port)\n    current_amount = np.zeros_like(desired_port)\n    prices = np.zeros_like(desired_port)\n\n    if algo.init:\n        positions_value = algo.portfolio.starting_cash\n    else:\n        positions_value = algo.portfolio.positions_value + \\\n            algo.portfolio.cash\n\n    for i, sid in enumerate(algo.sids):\n        current_amount[i] = algo.portfolio.positions[sid].amount\n        prices[i] = data[sid].price\n\n    desired_amount = np.round(desired_port * positions_value / prices)\n\n    algo.last_desired_port = desired_port\n    diff_amount = desired_amount - current_amount\n\n    for i, sid in enumerate(algo.sids):\n        algo.order(sid, diff_amount[i])\n\n\ndef simplex_projection(v, b=1):\n     Projection vectors to the simplex domain\n\n    Implemented according to the paper: Efficient projections onto the\n    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.\n    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg\n    Optimization Problem: min_{w}\\| w - v \\|_{2}^{2}\n    s.t. sum_{i=1}^{m}=z, w_{i}\\geq 0\n\n    Input: A vector v \\in R^{m}, and a scalar z   0 (default=1)\n    Output: Projection vector w\n\n    :Example:\n      proj = simplex_projection([.4 ,.3, -.4, .5])\n      print(proj)\n    array([ 0.33333333, 0.23333333, 0. , 0.43333333])\n      print(proj.sum())\n    1.0\n\n    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)\n    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).\n     \n\n    v = np.asarray(v)\n    p = len(v)\n\n    # Sort v into u in descending order\n    v = (v   0) * v\n    u = np.sort(v)[::-1]\n    sv = np.cumsum(u)\n\n    rho = np.where(u   (sv - b) / np.arange(1, p + 1))[0][-1]\n    theta = np.max([0, (sv[rho] - b) / (rho + 1)])\n    w = (v - theta)\n    w[w   0] = 0\n    return w\n\nstart = datetime(2004, 1, 1, 0, 0, 0, 0, pytz.utc)\nend = datetime(2010, 1, 1, 0, 0, 0, 0, pytz.utc)\ndata = load_from_yahoo(stocks=STOCKS, indexes={}, start=start, end=end)\ndata = data.dropna()\nolmar = TradingAlgorithm(handle_data=handle_data,\n                         initialize=initialize,\n                         identifiers=STOCKS)\nbacktest = olmar.run(data)  AMD\nCERN\nCOST\nDELL\nGPS\nINTC\nMMM\n[2015-08-05 20:16:25.197696] INFO: Performance: Simulated 1511 trading days out of 1511.\n[2015-08-05 20:16:25.198356] INFO: Performance: first open: 2004-01-02 14:31:00+00:00\n[2015-08-05 20:16:25.198812] INFO: Performance: last close: 2009-12-31 21:00:00+00:00",
            "title": "Run our zipline algorithm"
        },
        {
            "location": "/zipline_algo_example/#extract-metrics",
            "text": "Get the returns, positions, and transactions from the zipline backtest object.  returns, positions, transactions, gross_lev = pf.utils.extract_rets_pos_txn_from_zipline(backtest)",
            "title": "Extract metrics"
        },
        {
            "location": "/zipline_algo_example/#single-plot-example",
            "text": "Make one plot of the top 5 drawdown periods.  pf.plot_drawdown_periods(returns, top=5).set_xlabel('Date')  /opt/miniconda/lib/python2.7/site-packages/matplotlib/cbook.py:137: MatplotlibDeprecationWarning: The \"loc\" positional argument to legend is deprecated. Please use the \"loc\" keyword instead.\n  warnings.warn(message, mplDeprecation, stacklevel=1) matplotlib.text.Text at 0x7f645912e110",
            "title": "Single plot example"
        },
        {
            "location": "/zipline_algo_example/#full-tear-sheet-example",
            "text": "Create a full tear sheet for our algorithm. As an example, set the live start date to something arbitrary.  pf.create_full_tear_sheet(returns, positions=positions, transactions=transactions,\n                          gross_lev=gross_lev, live_start_date='2009-10-22')  Entire data start date: 2004-01-02 00:00:00+00:00\nEntire data end date: 2009-12-31 00:00:00+00:00\n\n\nOut-of-Sample Months: 2\nBacktest Months: 69\n                   Backtest  Out_of_Sample  All_History\nsortino_ratio          0.74           0.43         0.76\nomega_ratio            1.09           1.05         1.09\nmax_drawdown          -0.60          -0.07        -0.60\ncalmar_ratio           0.20           0.84         0.21\nannual_return          0.12           0.06         0.12\nstability              0.00           0.05         0.01\nsharpe_ratio           0.47           0.28         0.48\nannual_volatility      0.26           0.22         0.25\nalpha                  0.09          -0.11         0.09\nbeta                   0.81           1.18         0.81\n\nWorst Drawdown Periods\n   net drawdown in %                  peak date                valley date  \\\n0              59.50  2007-11-06 00:00:00+00:00  2008-11-20 00:00:00+00:00   \n1              22.33  2006-02-16 00:00:00+00:00  2006-08-31 00:00:00+00:00   \n2              12.52  2005-07-28 00:00:00+00:00  2005-10-12 00:00:00+00:00   \n3              11.28  2004-11-15 00:00:00+00:00  2005-04-28 00:00:00+00:00   \n4               9.44  2007-07-16 00:00:00+00:00  2007-08-06 00:00:00+00:00\n\n               recovery date duration  \n0                        NaN      NaN  \n1  2007-05-21 00:00:00+00:00      328  \n2  2006-01-11 00:00:00+00:00      120  \n3  2005-07-22 00:00:00+00:00      180  \n4  2007-09-04 00:00:00+00:00       37\n\n\n2-sigma returns daily    -0.032\n2-sigma returns weekly   -0.065\ndtype: float64\n\nStress Events\n          mean    min    max\nLehmann -0.003 -0.044  0.044\nAug07    0.003 -0.030  0.030\nSept08  -0.006 -0.043  0.040\n2009Q1  -0.004 -0.050  0.034\n2009Q2   0.007 -0.038  0.062\n\n\nTop 10 long positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nTop 10 short positions of all time (and max%)\n[]\n[]\n\n\nTop 10 positions of all time (and max%)\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]\n\n\nAll positions ever held\n[2 6 1 3 0 5 4]\n[ 0.993  0.911  0.845  0.717  0.709  0.666  0.62 ]",
            "title": "Full tear sheet example"
        },
        {
            "location": "/bayesian/",
            "text": "Bayesian performance analysis example in pyfolio\n\n\nThere are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics. \n\n\nThe main benefit of these methods is \nuncertainty quantification\n. All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.\n\n\nLets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in \nPyMC3\n to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in \ncreate_full_tear_sheet()\n).\n\n\nImport pyfolio\n\n\n%matplotlib inline\nimport pyfolio as pf\n\n\n\n\nUsing gpu device 0: NVS 5200M\n\n\n\nFetch the daily returns for a stock\n\n\nstock_rets = pf.utils.get_symbol_rets('FB')\n\n\n\n\nCreate Bayesian tear sheet\n\n\nout_of_sample = stock_rets.index[-40]\n\n\n\n\npf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample)\n\n\n\n\n [-----------------100%-----------------] 2000 of 2000 complete in 2.7 sec\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n  warnings.warn(\"No labelled objects found. \"\n\n\n\n\n\nLets go through these row by row:\n\n\n\n\nThe first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw at in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a \nStudent-T distribution\n with heavier tails.\n\n\nThe next row is comparing mean returns of the in-sample (backest) and OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. As you can see, the green distribution on the left side is much wider representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability \n 97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called \nBEST\n and was developed by John Kruschke.\n\n\nThe next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.\n\n\nThe 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.\n\n\nThe 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.\n\n\nLastly, a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.\n\n\n\n\nRunning models directly\n\n\nYou can also run individual models. All models can be found in \npyfolio.bayesian\n and run via the \nrun_model()\n function.\n\n\nhelp(pf.bayesian.run_model)\n\n\n\n\nHelp on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.\n\n\n\nFor example, to run a model that assumes returns to be normally distributed, you can call:\n\n\n# Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)\n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 0.9 sec\n\n\n\nThe returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are \n 0:\n\n\n# Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio \n 0 = {:3}%'.format((trace['sharpe'] \n 0).mean() * 100))\n\n\n\n\nProbability of Sharpe ratio \n 0 = 85.0%\n\n\n\nBut we can also interact with it like with any other \npymc3\n trace:\n\n\nimport pymc3 as pm\npm.traceplot(trace);\n\n\n\n\n\n\nFurther reading\n\n\nFor more information on Bayesian statistics, check out these resources:\n\n\n\n\nA blog post about the Bayesian models with Sepideh Sadeghi\n\n\nMy personal blog on Bayesian modeling\n\n\nA talk I gave in Singapore on \nProbabilistic Programming in Quantitative Finance\n\n\nThe IPython NB book \nBayesian Methods for Hackers\n.",
            "title": "Bayesian analysis"
        },
        {
            "location": "/bayesian/#bayesian-performance-analysis-example-in-pyfolio",
            "text": "There are also a few more advanced (and still experimental) analysis methods in pyfolio based on Bayesian statistics.   The main benefit of these methods is  uncertainty quantification . All the values you saw above, like the Sharpe ratio, are just single numbers. These estimates are noisy because they have been computed over a limited number of data points. So how much can you trust these numbers? You don't know because there is no sense of uncertainty. That is where Bayesian statistics helps as instead of single values, we are dealing with probability distributions that assign degrees of belief to all possible parameter values.  Lets create the Bayesian tear sheet. Under the hood this is running MCMC sampling in  PyMC3  to estimate the posteriors which can take quite a while (that's the reason why we don't generate this by default in  create_full_tear_sheet() ).",
            "title": "Bayesian performance analysis example in pyfolio"
        },
        {
            "location": "/bayesian/#import-pyfolio",
            "text": "%matplotlib inline\nimport pyfolio as pf  Using gpu device 0: NVS 5200M",
            "title": "Import pyfolio"
        },
        {
            "location": "/bayesian/#fetch-the-daily-returns-for-a-stock",
            "text": "stock_rets = pf.utils.get_symbol_rets('FB')",
            "title": "Fetch the daily returns for a stock"
        },
        {
            "location": "/bayesian/#create-bayesian-tear-sheet",
            "text": "out_of_sample = stock_rets.index[-40]  pf.create_bayesian_tear_sheet(stock_rets, live_start_date=out_of_sample)   [-----------------100%-----------------] 2000 of 2000 complete in 2.7 sec\n\n/home/wiecki/miniconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n  warnings.warn(\"No labelled objects found. \"   Lets go through these row by row:   The first one is the Bayesian cone plot that is the result of a summer internship project of Sepideh Sadeghi here at Quantopian. It's similar to the cone plot you already saw at in the tear sheet above but has two critical additions: (i) it takes uncertainty into account (i.e. a short backtest length will result in a wider cone), and (ii) it does not assume normality of returns but instead uses a  Student-T distribution  with heavier tails.  The next row is comparing mean returns of the in-sample (backest) and OOS (forward) period. As you can see, mean returns are not a single number but a (posterior) distribution that gives us an indication of how certain we can be in our estimates. As you can see, the green distribution on the left side is much wider representing our increased uncertainty due to having less OOS data. We can then calculate the difference between these two distributions as shown on the right side. The grey lines denote the 2.5% and 97.5% percentiles. Intuitively, if the right grey line is lower than 0 you can say that with probability   97.5% the OOS mean returns are below what is suggested by the backtest. The model used here is called  BEST  and was developed by John Kruschke.  The next couple of rows follow the same pattern but are an estimate of annual volatility, Sharpe ratio and their respective differences.  The 5th row shows the effect size or the difference of means normalized by the standard deviation and gives you a general sense how far apart the two distributions are. Intuitively, even if the means are significantly different, it may not be very meaningful if the standard deviation is huge amounting to a tiny difference of the two returns distributions.  The 6th row shows predicted returns (based on the backtest) for tomorrow, and 5 days from now. The blue line indicates the probability of losing more than 5% of your portfolio value and can be interpeted as a Bayesian VaR estimate.  Lastly, a Bayesian estimate of annual alpha and beta. In addition to uncertainty estimates, this model, like all above ones, assumes returns to be T-distributed which leads to more robust estimates than a standard linear regression would.",
            "title": "Create Bayesian tear sheet"
        },
        {
            "location": "/bayesian/#running-models-directly",
            "text": "You can also run individual models. All models can be found in  pyfolio.bayesian  and run via the  run_model()  function.  help(pf.bayesian.run_model)  Help on function run_model in module pyfolio.bayesian:\n\nrun_model(model, returns_train, returns_test=None, bmark=None, samples=500)\n    Run one of the Bayesian models.\n\n    Parameters\n    ----------\n    model : {'alpha_beta', 't', 'normal', 'best'}\n        Which model to run\n    returns_train : pd.Series\n        Timeseries of simple returns\n    returns_test : pd.Series (optional)\n        Out-of-sample returns. Datetimes in returns_test will be added to\n        returns_train as missing values and predictions will be generated\n        for them.\n    bmark : pd.Series (optional)\n        Only used for alpha_beta to estimate regression coefficients.\n        If bmark has more recent returns than returns_train, these dates\n        will be treated as missing values and predictions will be\n        generated for them taking market correlations into account.\n\n    Returns\n    -------\n    pymc3.sampling.BaseTrace object\n        A PyMC3 trace object that contains samples for each parameter\n        of the posterior.  For example, to run a model that assumes returns to be normally distributed, you can call:  # Run model that assumes returns to be T-distributed\ntrace = pf.bayesian.run_model('t', stock_rets)   [-----------------100%-----------------] 500 of 500 complete in 0.9 sec  The returned trace object can be directly inquired. For example might we ask what the probability of the Sharpe ratio being larger than 0 is by checking what percentage of posterior samples of the Sharpe ratio are   0:  # Check what frequency of samples from the sharpe posterior are above 0.\nprint('Probability of Sharpe ratio   0 = {:3}%'.format((trace['sharpe']   0).mean() * 100))  Probability of Sharpe ratio   0 = 85.0%  But we can also interact with it like with any other  pymc3  trace:  import pymc3 as pm\npm.traceplot(trace);",
            "title": "Running models directly"
        },
        {
            "location": "/bayesian/#further-reading",
            "text": "For more information on Bayesian statistics, check out these resources:   A blog post about the Bayesian models with Sepideh Sadeghi  My personal blog on Bayesian modeling  A talk I gave in Singapore on  Probabilistic Programming in Quantitative Finance  The IPython NB book  Bayesian Methods for Hackers .",
            "title": "Further reading"
        }
    ]
}